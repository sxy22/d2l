{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924145f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4f00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87288e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "#                                 '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "# d2l.download('time_machine', cache_dir='../../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f3772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 文本总行数: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open('../../data\\\\timemachine.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba795a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f31a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fbfbe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0), ('i', 1), ('and', 2), ('of', 3), ('a', 4), ('to', 5), ('was', 6), ('in', 7), ('that', 8), ('my', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e059b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6217e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbe029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3043a652",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "101c94d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_sequential(corpus, batch_size, num_steps, vocab_size):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1, vocab_size), Ys.reshape(batch_size, -1, vocab_size)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ef92163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fbc10c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(35):\n",
    "    lst.append([i+1, i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55444e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(lst, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7df52785",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "num_steps=5\n",
    "corpus = lst\n",
    "# corpus = list(range(35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ca5e6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = random.randint(0, num_steps)\n",
    "num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c502db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e242c029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2],\n",
       "         [ 2,  3],\n",
       "         [ 3,  4],\n",
       "         [ 4,  5],\n",
       "         [ 5,  6],\n",
       "         [ 6,  7],\n",
       "         [ 7,  8],\n",
       "         [ 8,  9],\n",
       "         [ 9, 10],\n",
       "         [10, 11],\n",
       "         [11, 12],\n",
       "         [12, 13],\n",
       "         [13, 14],\n",
       "         [14, 15],\n",
       "         [15, 16],\n",
       "         [16, 17],\n",
       "         [17, 18]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [19, 20],\n",
       "         [20, 21],\n",
       "         [21, 22],\n",
       "         [22, 23],\n",
       "         [23, 24],\n",
       "         [24, 25],\n",
       "         [25, 26],\n",
       "         [26, 27],\n",
       "         [27, 28],\n",
       "         [28, 29],\n",
       "         [29, 30],\n",
       "         [30, 31],\n",
       "         [31, 32],\n",
       "         [32, 33],\n",
       "         [33, 34],\n",
       "         [34, 35]]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.reshape(batch_size, -1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2fa1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "# 从随机偏移量开始划分序列\n",
    "offset = random.randint(0, num_steps)\n",
    "num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "num_batches = Xs.shape[1] // num_steps\n",
    "for i in range(0, num_steps * num_batches, num_steps):\n",
    "    X = Xs[:, i: i + num_steps]\n",
    "    Y = Ys[:, i: i + num_steps]\n",
    "    yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "39c71a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[[ 6,  7],\n",
      "         [ 7,  8],\n",
      "         [ 8,  9],\n",
      "         [ 9, 10],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[20, 21],\n",
      "         [21, 22],\n",
      "         [22, 23],\n",
      "         [23, 24],\n",
      "         [24, 25]]]) \n",
      "Y: tensor([[[ 7,  8],\n",
      "         [ 8,  9],\n",
      "         [ 9, 10],\n",
      "         [10, 11],\n",
      "         [11, 12]],\n",
      "\n",
      "        [[21, 22],\n",
      "         [22, 23],\n",
      "         [23, 24],\n",
      "         [24, 25],\n",
      "         [25, 26]]])\n",
      "X:  tensor([[[11, 12],\n",
      "         [12, 13],\n",
      "         [13, 14],\n",
      "         [14, 15],\n",
      "         [15, 16]],\n",
      "\n",
      "        [[25, 26],\n",
      "         [26, 27],\n",
      "         [27, 28],\n",
      "         [28, 29],\n",
      "         [29, 30]]]) \n",
      "Y: tensor([[[12, 13],\n",
      "         [13, 14],\n",
      "         [14, 15],\n",
      "         [15, 16],\n",
      "         [16, 17]],\n",
      "\n",
      "        [[26, 27],\n",
      "         [27, 28],\n",
      "         [28, 29],\n",
      "         [29, 30],\n",
      "         [30, 31]]])\n"
     ]
    }
   ],
   "source": [
    "for X, Y in seq_data_iter_sequential(lst, batch_size=2, num_steps=5, vocab_size=2):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b207b51",
   "metadata": {},
   "source": [
    "# rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3de6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c234083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2f986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bc6cd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 32])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(X.T.long(), 32).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8261d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 27],\n",
       "        [12, 28],\n",
       "        [13, 29],\n",
       "        [14, 30],\n",
       "        [15, 31]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1279962",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dee922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
